{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5123ec77-e8ac-4f59-b7f4-0e0f09b080b5",
   "metadata": {},
   "source": [
    "# Gradient descent based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9bce6-c7b8-4f02-9783-bea8d6376d86",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0639c-7577-438f-8530-2c1b8ff4a175",
   "metadata": {},
   "source": [
    "In these models, we assign a **Weight** for each feature in a linear fashion and add these values together with some **Bias** to reach our target value. but how do we choose proper weights and bias?<br>\n",
    "for that, we have to define **Cost/Loss Function**. there are various cost functions in machine learning but in the end, all of them somehow represent our accuracy of prediction, so by reducing cost function we expect to reach more accurate prediction (we are talking about training phase accuracy and not test phase accuracy). so we need a mechanism to minimize our cost function, for that, we are using **Gradient Descent algorithm** (that's why we call these models gradient descent-based models).<br>\n",
    "before we talk about gradient descent, it is worth mentioning that we said we deal with linear combinations of features and if we only stick with them we only can predict problems with linear answers. so there are several ways to add some **non-linearity** to our predictor equations for example adding some polynomial terms of features or using other non-linear functions like **Sigmoid**, **ReLU**, etc (more on them later). now let's talk about gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b39224-e801-41c0-9290-443101450d82",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba7ab6-19f5-43f8-b7dc-f923dcd5dfd9",
   "metadata": {},
   "source": [
    "in these models, we assign a **Weight** for each feature in a linear fashion and add these values together with some **Bias** to reach our target value. but how do we choose proper weights and bias?<br>\n",
    "for that, we have to define **Cost/Loss Function**. there are various cost functions in machine learning but in the end, all of them somehow represent our accuracy of prediction, so the lower the cost function the Gradient Descent (GD) is an iterative first-order optimization algorithm that is used to find the local optimum of a given function. here our function is the cost/loss function. as we said before the cost function somehow represents our prediction accuracy so loss equal to zero is our goal. because of the derivative nature of the GD algorithm the cost function needs to have two specifications, it has to be:\n",
    "1. Differentiable\n",
    "2. Convex/Concave<br>\n",
    "\n",
    "as we mentioned before the GD is an iterative process and in each iteration by using derivatives, it changes our parameters until we reach a certain number of iterations (maximum iteration) or the difference between loss value reaches a certain small value (tolerance).<br>\n",
    "this method has a parameter called **Learning rate**, which will multiply to our derivative value before we update our parameters and have a value between zero and one. the small value of the learning rate means small steps toward optimum value and a big learning rate means big steps. so we have to deal with it as hyperparameters in our problems.<br>\n",
    "In general GD algorithm steps are:\n",
    "1. initiation of our parameters (starting point)\n",
    "2. calculate the gradient at this point\n",
    "3. make a scaled step (with learning rate) in the opposite direction to the gradient (if we want to minimize)\n",
    "4. repeat steps 2 and 3 until we reach our maximum iteration or tolerance<br>\n",
    "\n",
    "there are various types of gradient descent and we can classify them like this:\n",
    "* From **Data Perspective**:\n",
    "    * **Batch** gradient descent: process all the training samples for each iteration.\n",
    "    * **Stochastic** gradient descent: process one sample at each iteration.\n",
    "    * **Mini-batch** gradient descent: process b samples for each iteration (b < number of samples)\n",
    "* From **Algorithm Perspective**\n",
    "    * **Momentum**-based gradient\n",
    "    * Root mean squared propagation or **RMSProp**\n",
    "    * Adaptive moment estimation or **ADAM**<br>\n",
    "\n",
    "By using all of these algorithms we will add hyperparameters to our problem. so we will have:\n",
    "* batch size ${b}$ for mini-batch GD. the default value is 32 but we can choose other values too (usually choose $2^{n}$ values).\n",
    "* $\\beta_1$ for momentum (default 0.9)\n",
    "* $\\beta_2$ and $\\epsilon$ for RMSProp (default 0.99 and 10e-8)\n",
    "* $\\beta_1$, $\\beta_2$, $\\epsilon$ for ADAM<br>\n",
    "\n",
    "Several models use gradient descent but the common ones are:\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Neural Network\n",
    "\n",
    "Now that we know about all of this stuff let's code them. but instead of using libraries, we going to code logistic regression from scratch first and after that learn how to implement them by using libraries.<br>\n",
    "but why from scratch? because this way we are going to understand the logic behind all the gradient descent-based models.<br>\n",
    "why logistic regression? because it is the base of all the gradient descent-based models.<br>\n",
    "why not linear regression? logistic regression and linear regression have a lot in common but logistic regression has a non-linearity in it, so let's learn how to use them too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c223a6-35bc-4106-8c89-a497c37de400",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc52ee-38a8-41eb-86c0-822892e42309",
   "metadata": {},
   "source": [
    "Logistic regression is a gradient descent based model that used for classification problems and it will estimates the probability of each class in our problem. as we discuss before each feature has its assign weight and we we will add them toghether in linear fashion with some bias term to calculate our prediction. but in order to get probability, we need some function to convert our prediction into probability or in other word a number between 0 and 1. so for that we use **Logistic function**. the two common loss function used for logistic regression are **logistic loss** function and **cross-entropy loss** function.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49568a-9c3e-457b-8ecb-2afe1dd53007",
   "metadata": {},
   "source": [
    "### Implementing Logistic Regression From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52cf70-e78b-4b12-af89-f3de706ee284",
   "metadata": {},
   "source": [
    "because i think psudo code is very important in coding process, first i will write psudo code for logistic regression with some notes to help us in coding process, then we will write our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478db2be-5bdd-49d0-abca-3a8d5298e13c",
   "metadata": {},
   "source": [
    "#### Psudo code for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5abd0-1394-45b6-a565-b687bd87b088",
   "metadata": {},
   "source": [
    "1. initializing weights (${w}$) and bias (${b}$).\n",
    "2. Calculate the linear combination. (${z} = w^{T}{X} + {b}$)\n",
    "3. perform the logistic function on z. (${a} = logistic({z})$). this is our estimation (prediction).\n",
    "4. calculating loss using proper formula. (here we will be using the cross-entropy loss function). our loss is function of y and a (${J} = f({y},{a})$).\n",
    "5. calculating our derivatives (da, dz, dw, db). here we need some calculus and particularly chain rule. In the end, we need dw, db but we have to calculate da, dz to get our result.\n",
    "    * $da = \\frac{\\partial J\\}{\\partial a\\}\\$<br>\n",
    "    \n",
    "    * $dz = \\frac{\\partial J\\}{\\partial z\\}\\ = \\frac{\\partial J\\}{\\partial a\\}\\ * \\frac{\\partial a\\}{\\partial z\\}\\$\n",
    "    \n",
    "    * $dw = \\frac{\\partial J\\}{\\partial w\\}\\ = \\frac{\\partial J\\}{\\partial a\\}\\ * \\frac{\\partial a\\}{\\partial z\\}\\ * \\frac{\\partial z\\}{\\partial w\\}\\$\n",
    "    \n",
    "    * $db = \\frac{\\partial J\\}{\\partial b\\}\\ = \\frac{\\partial J\\}{\\partial a\\}\\ * \\frac{\\partial a\\}{\\partial z\\}\\ * \\frac{\\partial z\\}{\\partial b\\}\\$<br>\n",
    "\n",
    "    \n",
    "    \n",
    "6. update w, b using dw, db.\n",
    "    * ${w} := {w} - \\alpha{dw}$\n",
    "    * ${b} := {b} - \\alpha{db}$\n",
    "7. repeat steps 2-6 until we reach our maximum iteration or our tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ada5c5-a85c-4e16-a58f-79ca42b6d6cd",
   "metadata": {},
   "source": [
    "#### Some note for implementing logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf88c86-97c1-4f3a-bdda-54b31820a83a",
   "metadata": {},
   "source": [
    "1. because we are dealing with matrices, the shape is important, every time we use `.T` or `.reshape`, we are trying to control our matrix shape for further calculation. so don't distract by them.\n",
    "2. here we will be using basic gradient descent, not other variants.\n",
    "3. $\\alpha\\$ is our learning rate.\n",
    "4. for initialization we can assign our parameters to zero or any random number (better be a small number between zero and 1 or some scale near it).\n",
    "5. for cross-entropy loss function:<br>\n",
    "\n",
    "    $\\frac{\\partial J\\}{\\partial a\\}\\ = - \\frac{y}{a}\\ + \\frac{1 - y}{1 - a}\\$\n",
    "6. for sigmoid function: <br>\n",
    "\n",
    "    $\\frac{\\partial a\\}{\\partial z\\}\\ = \\{a}{(1 - a)}\\$<br>\n",
    "7. so:\n",
    "    \n",
    "    $\\frac{\\partial J\\}{\\partial z\\}\\ = \\{a} - {y}\\$\n",
    "\n",
    "8. as we said before our output in logistic regression is the probability so it will be between zero and one, in order to use it for classifying we will change our output to 0 and 1, each represents one of our classes. so if our estimation is bigger than some **threshold** (default 0.5), we classify our answer as 1, and if it's lower than our threshold we will classify it as 0. we also can treat it as hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a040e-ebc7-4f1a-a6d2-a640abb97598",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d689b82-3fd2-40f7-8624-15d2343be372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from scipy.special import expit #our logistic function\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986a93b7-ac11-4ead-a04c-55831c68d2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing our data\n",
    "data, target = load_breast_cancer(return_X_y=True)\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12e1c30-5f9c-4f66-9887-614eaf5ee477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting our data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66288b4-2b0f-4f74-ac32-edaac63ab317",
   "metadata": {},
   "outputs": [],
   "source": [
    "##scaling our data, remember we only fit on our train data\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cb29ee-62d1-4071-820b-f3f058116141",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make sure we have numpy array in our desire shape\n",
    "X = np.array(X).T\n",
    "Y = np.array(y_train)\n",
    "n = len(X) # number of features\n",
    "m = len(Y) # number of training samples\n",
    "## reshaping our data to desire shape and initialize our weights and bias (step 1)\n",
    "Y = Y.reshape((1,m))\n",
    "b = np.zeros((1,m))\n",
    "w = np.zeros((n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa3c41c-da6b-4ee3-8975-59216bf2fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "alpha = 0.025 #learning rate\n",
    "iteration = 700 # number of iteration\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824a9024-39cc-418e-9e59-3345773ccde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## our iterative process for calculate w , b\n",
    "for i in range(iteration): #(step 7)\n",
    "    Z = np.dot(w.T,X)+b # calculate linear combination (step 2)\n",
    "    A = expit(Z) # (step 3) adding our non linearity here for binary classification the proper one is logistic function that's why we call it logistic regression\n",
    "    loss = -Y*np.log(A)+(1-Y)*np.log(1-A) # calculating loss with proper formula (cross entropy) (step 4)\n",
    "    J = np.sum(loss) # loss over all samples (step 4)\n",
    "    ## computing gradients(step 5)\n",
    "    dZ = A-Y \n",
    "    dw = (np.dot(X,dZ.T))/m\n",
    "    db = (np.sum(dZ))/m\n",
    "    ## updating (step 6)\n",
    "    w = w-alpha*dw\n",
    "    b = b-alpha*db\n",
    "#     plt.scatter(i,np.abs(J))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d690942-15c7-44b6-a02b-9780cfec28cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8647747718188667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loss value\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "869ed450-9924-4072-a65e-26b2d6be574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9868131868131869"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## training accuracy\n",
    "A[A>=threshold] = 1\n",
    "A[A<threshold] = 0\n",
    "train_accuracy = np.sum(A==Y)/len(A.T)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebcf6e8f-ecc7-4d6d-8e06-91235eecf46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing testing data to proper input format\n",
    "X_test = np.array(X_test).T\n",
    "Y_test = np.array(y_test)\n",
    "Y_test = Y_test.reshape((1,len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c36707-3442-4e7d-9566-fad9d56db880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test accuracy\n",
    "Z_predict = np.dot(w.T,X_test)\n",
    "Y_predict = expit(Z_predict)\n",
    "Y_predict[Y_predict>=threshold] = 1\n",
    "Y_predict[Y_predict<threshold] = 0\n",
    "test_accuracy = np.sum(Y_predict==Y_test)/len(Y_predict.T)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85802ea-e6d4-4d8c-bb26-46c27c7f76af",
   "metadata": {},
   "source": [
    "**NOTE:** if we want to implement linear regression, we just need to use the least square function for loss and we don't need the logistic function, so the derivatives will be changed, and actually, they will be much simpler, that's why I choose to write logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86907651-4664-4c3f-8677-59c740c9804c",
   "metadata": {},
   "source": [
    "### GDB Models in Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd7867-4dc0-4143-9d4d-116a7969666d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096ef34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af103253",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900571c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73218f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbbf4385",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cce3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15774d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc13f676-583b-4598-a844-3e5c3d8781ff",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3fa4c-bb86-4484-a877-2c95303139f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0ced8-dd6a-4467-b95b-5003d44b98ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3f2d6-7c6c-47ba-a093-5e90602ace85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e05b8-9c06-4c38-b6c5-7c2e5eb3e282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5a24e-5dbc-4389-b1f2-ff0f4ca87b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d643bd1-95e1-4220-bf52-97364ee16961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7316d-0d8e-4fdc-8b9b-8361f36734b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6fb1e08-b56e-48eb-b4c6-1386e0c95831",
   "metadata": {},
   "source": [
    "### Implementing neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac7a46-aa82-4fd7-8d88-67f29cdb4cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf9f04c-ee9a-469f-a179-40048f3b3e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda01ff-ae5a-49e3-9233-7e5677eddba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c00139-f3ca-4267-9690-cd04681acf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fe7d5c6",
   "metadata": {},
   "source": [
    "### Implementing neural network with pytorch( or sth else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639347c-be90-47a7-8f48-19e78857f3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ccf34-4997-44ba-817d-c8edf56c2598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ce981-bb64-4344-8f18-63a2787696ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7bd66-b25f-4cb0-b326-97da8c667126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
